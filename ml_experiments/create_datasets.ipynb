{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682e3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dce3813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_accident_dataset(root, train_ratio=0.4, validation_ratio=0.2):\n",
    "    \n",
    "    random.seed(42)\n",
    "\n",
    "    all_data_dir = os.path.join(root, 'all_data')\n",
    "    no_accident_dir = os.path.join(all_data_dir, '0_no_accident')\n",
    "    accident_dir = os.path.join(all_data_dir, '1_accident')\n",
    "\n",
    "    \n",
    "    # Read the data from the accident and no_accident directories\n",
    "    accident_data = []\n",
    "    for filename in os.listdir(accident_dir):\n",
    "        img_path = os.path.join(accident_dir, filename)\n",
    "        with Image.open(img_path) as img:\n",
    "            np_img = np.array(img)\n",
    "            accident_data.append((np_img, 1))\n",
    "\n",
    "    no_accident_data = []\n",
    "    for filename in os.listdir(no_accident_dir):\n",
    "        img_path = os.path.join(no_accident_dir, filename)\n",
    "        with Image.open(img_path) as img:\n",
    "            np_img = np.array(img)\n",
    "            no_accident_data.append((np_img, 0))\n",
    "    \n",
    "    print(\"No Accident :: 0\")\n",
    "    print(\"Accident :: 1\" )\n",
    "    \n",
    "    \n",
    "    all_data = accident_data + no_accident_data\n",
    "    random.shuffle(all_data)\n",
    "\n",
    "    # Split the data into train, validation and test sets\n",
    "    train_split_index = int(len(all_data) * train_ratio)\n",
    "    validation_split_index = train_split_index + int(len(all_data) * validation_ratio)\n",
    "        \n",
    "    train_data = all_data[ : train_split_index]\n",
    "    validation_data = all_data[train_split_index : validation_split_index]\n",
    "    test_data = all_data[validation_split_index : ]\n",
    "\n",
    "    # Save the datasets as pickle files\n",
    "    with open(os.path.join(root, 'accident_train_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "\n",
    "    with open(os.path.join(root, 'accident_validation_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(validation_data, f)\n",
    "        \n",
    "    with open(os.path.join(root, 'accident_test_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(test_data, f)\n",
    "        \n",
    "    \n",
    "    return train_data, validation_data, test_data\n",
    "        \n",
    "def split_cifar_dataset(root, total_cnt=7000, train_ratio=0.6, validation_ratio=0.2):\n",
    "    \n",
    "    random.seed(42)\n",
    "\n",
    "    cifar_dir = os.path.join(root, 'all_data/2_cifar7')\n",
    "\n",
    "    label = 2\n",
    "    cifar_data = []\n",
    "    for folder in os.listdir(cifar_dir):\n",
    "        \n",
    "        folder_dir = os.path.join(cifar_dir, folder)\n",
    "        \n",
    "        if(os.path.isdir(folder_dir) == False):\n",
    "            continue\n",
    "        \n",
    "        print(folder_dir, \" :: \", label)\n",
    "        \n",
    "        for filename in os.listdir(folder_dir):\n",
    "            img_path = os.path.join(folder_dir, filename)\n",
    "            with Image.open(img_path) as img:\n",
    "                np_img = np.array(img)\n",
    "                cifar_data.append((np_img, label))\n",
    "        label += 1\n",
    "\n",
    "    random.shuffle(cifar_data)\n",
    "    \n",
    "    cifar_data = cifar_data[:total_cnt]\n",
    "    \n",
    "\n",
    "    # Split the data into train, validation and test sets\n",
    "    train_split_index = int(len(cifar_data) * train_ratio)\n",
    "    validation_split_index = train_split_index + int(len(cifar_data) * validation_ratio)\n",
    "\n",
    "    train_data = cifar_data[:train_split_index]\n",
    "    validation_data = cifar_data[train_split_index : validation_split_index]\n",
    "    test_data = cifar_data[validation_split_index:]\n",
    "\n",
    "    # Save the datasets as pickle files\n",
    "    with open(os.path.join(root, 'cifar_train_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "\n",
    "    with open(os.path.join(root, 'cifar_validation_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(validation_data, f)\n",
    "    \n",
    "    with open(os.path.join(root, 'cifar_test_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(test_data, f)\n",
    "        \n",
    "    return train_data, validation_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8339505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Accident :: 0\n",
      "Accident :: 1\n"
     ]
    }
   ],
   "source": [
    "accident_train_data, accident_validation_data, accident_test_data = split_accident_dataset(root=\"crime_canary_dataset\", train_ratio=0.6, validation_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f8d848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime_canary_dataset/all_data/2_cifar7/cat  ::  2\n",
      "crime_canary_dataset/all_data/2_cifar7/dog  ::  3\n",
      "crime_canary_dataset/all_data/2_cifar7/bird  ::  4\n",
      "crime_canary_dataset/all_data/2_cifar7/ship  ::  5\n",
      "crime_canary_dataset/all_data/2_cifar7/frog  ::  6\n",
      "crime_canary_dataset/all_data/2_cifar7/horse  ::  7\n",
      "crime_canary_dataset/all_data/2_cifar7/deer  ::  8\n"
     ]
    }
   ],
   "source": [
    "tot_cifar_data = 7000\n",
    "tot_accident_data = len(accident_train_data) + len(accident_validation_data) + len(accident_test_data)\n",
    "\n",
    "cifar_train_ratio = (len(accident_train_data))/tot_accident_data\n",
    "cifar_validation_ratio = (len(accident_validation_data))/tot_accident_data\n",
    "\n",
    "cifar_train_data, cifar_validation_data, cifar_test_data = split_cifar_dataset(root=\"crime_canary_dataset\", total_cnt=int(tot_cifar_data), train_ratio=cifar_train_ratio, validation_ratio=cifar_validation_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6aaf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d325fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92806c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_3_cat_datasets(root_dir, train_ratio=0.6, val_ratio=0.2):\n",
    "    \n",
    "    random.seed(42)\n",
    "    \n",
    "    main_folder = 'accident_dataset'\n",
    "    \n",
    "    all_datasets = {}\n",
    "    folders = ['accident_detection', 'accident_severity', 'vehicles_in_accidents']\n",
    "    for folder in folders:\n",
    "        path = os.path.join(root_dir, main_folder)\n",
    "        path = os.path.join(path, folder)\n",
    "        subfolders = sorted(os.listdir(path))\n",
    "        labels = {subfolder: i for i, subfolder in enumerate(subfolders)}\n",
    "        \n",
    "        print(folder)\n",
    "        print(\"------------------------\")\n",
    "        print(labels)\n",
    "        print()\n",
    "    \n",
    "        train_data = []\n",
    "        val_data = []\n",
    "        test_data = []\n",
    "\n",
    "        for subfolder, label in labels.items():\n",
    "            subfolder_path = os.path.join(path, subfolder)\n",
    "         \n",
    "            files = os.listdir(subfolder_path)\n",
    "            random.shuffle(files)\n",
    "\n",
    "            n = len(files)\n",
    "            n_train = int(train_ratio * n)\n",
    "            n_val = int(val_ratio * n)\n",
    "            n_test = n - n_train - n_val\n",
    "\n",
    "            # Create the train dataset\n",
    "            for file in files[:n_train]:\n",
    "                img_path = os.path.join(subfolder_path, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    np_img = np.array(img)\n",
    "                    train_data.append((np_img, label))\n",
    "\n",
    "            # Create the validation dataset\n",
    "            for file in files[n_train:n_train + n_val]:\n",
    "                img_path = os.path.join(subfolder_path, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    np_img = np.array(img)\n",
    "                    val_data.append((np_img, label))\n",
    "\n",
    "            # Create the test dataset\n",
    "            for file in files[n_train + n_val:]:\n",
    "                img_path = os.path.join(subfolder_path, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    np_img = np.array(img)\n",
    "                    test_data.append((np_img, label))\n",
    "\n",
    "        with open(os.path.join(root_dir, folder + '_train.pkl'), 'wb') as f:\n",
    "            random.shuffle(train_data)\n",
    "            pickle.dump(train_data, f)\n",
    "        with open(os.path.join(root_dir, folder + '_validation.pkl'), 'wb') as f:\n",
    "            random.shuffle(val_data)\n",
    "            pickle.dump(val_data, f)\n",
    "        with open(os.path.join(root_dir, folder + '_test.pkl'), 'wb') as f:\n",
    "            random.shuffle(test_data)\n",
    "            pickle.dump(test_data, f)\n",
    "\n",
    "        all_datasets[folder + '_train'] = train_data\n",
    "        all_datasets[folder + '_validation'] = val_data\n",
    "        all_datasets[folder + '_test'] = test_data\n",
    "\n",
    "    return all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dae57ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accident_detection\n",
      "------------------------\n",
      "{'0_no_accident': 0, '1_accident': 1}\n",
      "\n",
      "accident_severity\n",
      "------------------------\n",
      "{'0_low': 0, '1_medium': 1, '2_high': 2}\n",
      "\n",
      "vehicles_in_accidents\n",
      "------------------------\n",
      "{'0_motorcycle': 0, '1_light': 1, '2_heavy': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_datasets = split_3_cat_datasets(root_dir=\"datasets_for_comparison\", train_ratio=0.6, val_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf49d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rpienv",
   "language": "python",
   "name": "rpienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
